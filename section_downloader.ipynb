{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kWdKSpnVUDkT"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class extractor():\n",
        "    def __init__(self):\n",
        "        self.res = {}\n",
        "    \n",
        "    def _get_content_(self, link):\n",
        "        link = \"https://www.sec.gov/Archives/\" + link\n",
        "        webpage_response = requests.get(link,headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        if webpage_response.status_code == 200: \n",
        "        # The HTTP 200 OK success status response code indicates that the request has succeeded. \n",
        "            body = webpage_response.text\n",
        "        else:\n",
        "            print(\"Unable to get response with Code : %d \" % (webpage_response.status_code))\n",
        "        return body\n",
        "\n",
        "    def _parse_content_(self, content, sec):\n",
        "        # Regex to find <DOCUMENT> tags\n",
        "        doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
        "        doc_end_pattern = re.compile(r'</DOCUMENT>')\n",
        "        # Regex to find <TYPE> tag prceeding any characters, terminating at new line\n",
        "        type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
        "\n",
        "        doc_start_is = [x.end() for x in doc_start_pattern.finditer(content)]\n",
        "        doc_end_is = [x.start() for x in doc_end_pattern.finditer(content)]\n",
        "        doc_types = [x[len('<TYPE>'):] for x in type_pattern.findall(content)]\n",
        "        # Create a loop to go through each section type and save only the 10-K section in the dictionary\n",
        "        content_n = ''\n",
        "        for doc_type, doc_start, doc_end in zip(doc_types, doc_start_is, doc_end_is):\n",
        "            if doc_type == '10-K':\n",
        "                content_n += content[doc_start:doc_end]\n",
        "        # Write the regex\n",
        "        regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1A|1B|1|2|3|4|5|7A|7|8)\\.{0,1})|(ITEM(\\s|&#160;|&nbsp;)(1A|1B|1|2|3|4|5|7A|7|8))')\n",
        "\n",
        "        # Use finditer to math the regex\n",
        "        matches = regex.finditer(content_n)\n",
        "        # Create the dataframe\n",
        "        test_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
        "        test_df.columns = ['item', 'start', 'end']\n",
        "        test_df['item'] = test_df.item.str.lower()\n",
        "\n",
        "        # Get rid of unnesesary charcters from the dataframe\n",
        "        test_df.replace('&#160;',' ',regex=True,inplace=True)\n",
        "        test_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
        "        test_df.replace(' ','',regex=True,inplace=True)\n",
        "        test_df.replace('\\.','',regex=True,inplace=True)\n",
        "        test_df.replace('>','',regex=True,inplace=True)\n",
        "\n",
        "\n",
        "        # Drop duplicates\n",
        "        count1a = len(test_df.loc[test_df['item']=='item1a'])\n",
        "        if count1a > 1:\n",
        "            pos_dat = test_df.sort_values('start', ascending=True)[10:].drop_duplicates(subset=['item'], keep='first')\n",
        "        else:\n",
        "            pos_dat = test_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='first')\n",
        "\n",
        "        # Set item as the dataframe index\n",
        "        pos_dat.set_index('item', inplace=True)\n",
        "\n",
        "        firm_dict = {}\n",
        "        item_lst = ['item1','item1a','item1b','item2','item3','item4','item5','item7','item7a','item8']\n",
        "        for i in sec:\n",
        "            if i == 'item8' or i not in item_lst:\n",
        "                print(\"Item out of index\")\n",
        "                continue\n",
        "            sec_content = content_n[pos_dat['start'].loc[i]:pos_dat['start'].loc[item_lst[item_lst.index(i)+1]]]\n",
        "            firm_dict[i] = self._prettify_(sec_content)\n",
        "        return firm_dict\n",
        "\n",
        "    def _prettify_(self,text):\n",
        "        parsed_content = BeautifulSoup(text, 'lxml')\n",
        "        text_part = parsed_content.get_text(\"\\n\\n\")\n",
        "        return text_part\n",
        "\n",
        "    def extract(self, cik_lst,link_lst, sec_lst):     # sec:item1a, item1, item1b\n",
        "        for i in range(len(link_lst)):\n",
        "            self.res[cik_lst[i]] = self._parse_content_(self._get_content_(link_lst[i]),sec_lst)\n",
        "        return self.res"
      ],
      "metadata": {
        "id": "HXiivS0dUZWN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "exampleset = [('0000001800',\n",
        "  'ABBOTT LABORATORIES',\n",
        "  '10-K',\n",
        "  'edgar/data/1800/0001047469-19-000624.txt'),\n",
        " ('0000796343',\n",
        "  'ADOBE INC.',\n",
        "  '10-K',\n",
        "  'edgar/data/796343/0000796343-19-000019.txt'),\n",
        " ('0000002488',\n",
        "  'ADVANCED MICRO DEVICES INC',\n",
        "  '10-K',\n",
        "  'edgar/data/2488/0000002488-19-000011.txt'),\n",
        " ('0000764180',\n",
        "  'ALTRIA GROUP, INC.',\n",
        "  '10-K',\n",
        "  'edgar/data/764180/0000764180-19-000023.txt')]'''"
      ],
      "metadata": {
        "id": "9vRXvEMOMQgi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# test case\n",
        "cik_lst = [i[0] for i in exampleset]\n",
        "link_lst = [i[3] for i in exampleset]\n",
        "sec_lst = ('item1','item1a') # attention: section list should be in tuples\n",
        "\n",
        "extr = extractor()\n",
        "test_res = extr.extract(cik_lst,link_lst,sec_lst)\n",
        "'''"
      ],
      "metadata": {
        "id": "mQxdOI06ahiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}